{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- Colab setup (run this cell first) ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install dependencies (quiet)\n",
        "!pip -q install pandas requests openpyxl fuzzywuzzy python-Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu8rsoAtspZK",
        "outputId": "f5c2099f-cc26-466c-da91-38b39bf5dd88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1) Configuration\n",
        "# ============================================================\n",
        "\n",
        "import io\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from fuzzywuzzy import fuzz, process\n",
        "\n",
        "# ----------------------------\n",
        "# User settings\n",
        "# ----------------------------\n",
        "REBUILD_FROM_SOURCE = False         # True = rebuild caches from Title II URLs / IPEDS file\n",
        "FUZZY_MATCH_THRESHOLD = 80\n",
        "\n",
        "# Where to write caches and outputs in Google Drive\n",
        "PROJECT_DIR = Path(\"/content/drive/MyDrive/T2_IPEDS_Crosswalk\")\n",
        "RAW_DIR = PROJECT_DIR / \"data\" / \"raw\"\n",
        "CACHE_DIR = PROJECT_DIR / \"data\" / \"cache\"\n",
        "OUTPUT_DIR = PROJECT_DIR / \"data\" / \"outputs\"\n",
        "for d in (RAW_DIR, CACHE_DIR, OUTPUT_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TITLEII_CACHE = CACHE_DIR / \"titleII_program_universe_2013_2024.csv\"\n",
        "IPEDS_MATCH_CACHE = CACHE_DIR / \"ipeds_match_table.csv\"\n",
        "\n",
        "CROSSWALK_OUT = OUTPUT_DIR / \"titleII_ipeds_crosswalk.csv\"\n",
        "UNMATCHED_OUT = OUTPUT_DIR / \"titleII_unmatched_for_manual_review.csv\"\n",
        "\n",
        "# ----------------------------\n",
        "# Official Title II source URLs\n",
        "# ----------------------------\n",
        "TITLEII_URLS = [\n",
        "    \"https://title2.ed.gov/Public/DataTools/2012/AllStates.xlsx\",\n",
        "    \"https://title2.ed.gov/Public/DataTools/2013/AllStates.xlsx\",\n",
        "    \"https://title2.ed.gov/Public/DataTools/2014/AllStates.xlsx\",\n",
        "    \"https://title2.ed.gov/Public/DataTools/2015/AllStates.xlsx\",\n",
        "    \"https://title2.ed.gov/Public/DataTools/2016/AllStates.xlsx\",\n",
        "    \"https://title2.ed.gov/Public/DataTools/2017/AllStates.xlsx\",\n",
        "    \"https://title2.ed.gov/Public/DataTools/2018/AllStates.xlsx\",\n",
        "    \"https://title2.ed.gov/Public/DataTools/2019/AllStates.xlsx\",\n",
        "    \"https://title2.ed.gov/Public/DataTools/2020/AllStates.xlsx\",\n",
        "    \"https://title2.ed.gov/Public/DataTools/2021/AllStates.xlsx\",\n",
        "    \"https://title2.ed.gov/Public/DataTools/2022/AllStates.xlsx\",\n",
        "    \"https://title2.ed.gov/Public/DataTools/2023/AllStates.xlsx\",\n",
        "    \"https://title2.ed.gov/Public/DataTools/2024/AllStates.xlsx\",\n",
        "]\n",
        "TITLEII_SHEET_NAME = \"Program\"\n",
        "\n",
        "TARGET_PROGRAM_TYPES = [\n",
        "    \"Traditional\",\n",
        "    \"Alternative, not IHE-based\",\n",
        "    \"Alternative, IHE-based\",\n",
        "]\n",
        "\n",
        "# ----------------------------\n",
        "# State / Territory mapping (Title II full names -> abbreviations)\n",
        "# ----------------------------\n",
        "STATE_NAME_TO_ABBR = {\n",
        "    \"Alabama\": \"AL\", \"Alaska\": \"AK\", \"Arizona\": \"AZ\", \"Arkansas\": \"AR\", \"California\": \"CA\",\n",
        "    \"Colorado\": \"CO\", \"Connecticut\": \"CT\", \"Delaware\": \"DE\", \"Florida\": \"FL\", \"Georgia\": \"GA\",\n",
        "    \"Hawaii\": \"HI\", \"Idaho\": \"ID\", \"Illinois\": \"IL\", \"Indiana\": \"IN\", \"Iowa\": \"IA\",\n",
        "    \"Kansas\": \"KS\", \"Kentucky\": \"KY\", \"Louisiana\": \"LA\", \"Maine\": \"ME\", \"Maryland\": \"MD\",\n",
        "    \"Massachusetts\": \"MA\", \"Michigan\": \"MI\", \"Minnesota\": \"MN\", \"Mississippi\": \"MS\", \"Missouri\": \"MO\",\n",
        "    \"Montana\": \"MT\", \"Nebraska\": \"NE\", \"Nevada\": \"NV\", \"New Hampshire\": \"NH\", \"New Jersey\": \"NJ\",\n",
        "    \"New Mexico\": \"NM\", \"New York\": \"NY\", \"North Carolina\": \"NC\", \"North Dakota\": \"ND\", \"Ohio\": \"OH\",\n",
        "    \"Oklahoma\": \"OK\", \"Oregon\": \"OR\", \"Pennsylvania\": \"PA\", \"Rhode Island\": \"RI\", \"South Carolina\": \"SC\",\n",
        "    \"South Dakota\": \"SD\", \"Tennessee\": \"TN\", \"Texas\": \"TX\", \"Utah\": \"UT\", \"Vermont\": \"VT\",\n",
        "    \"Virginia\": \"VA\", \"Washington\": \"WA\", \"West Virginia\": \"WV\", \"Wisconsin\": \"WI\", \"Wyoming\": \"WY\",\n",
        "    \"District of Columbia\": \"DC\",\n",
        "    \"American Samoa\": \"AS\", \"Guam\": \"GU\", \"Northern Mariana Islands\": \"MP\", \"Puerto Rico\": \"PR\",\n",
        "    \"Virgin Islands\": \"VI\",\n",
        "    \"Marshall Islands\": \"MH\", \"Micronesia\": \"FM\",\n",
        "}\n",
        "\n",
        "# ----------------------------\n",
        "# IPEDS input file (place your XLSX here)\n",
        "# ----------------------------\n",
        "IPEDS_MAIN_FILE = RAW_DIR / \"IPEDS_Institution_Reference_Table.xlsx\"\n",
        "\n",
        "# Column names expected in your IPEDS CSV\n",
        "IPEDS_UNITID_COL = \"UnitID\"\n",
        "IPEDS_OFFICIAL_NAME_COL = \"Institution Name\"\n",
        "IPEDS_ENTITY_NAME_COL = \"Institution (entity) name (HD2023)\"\n",
        "IPEDS_ALIAS_SOURCE_COL = \"Institution name alias (HD2023)\"\n",
        "IPEDS_STATE_ABBR_COL = \"State abbreviation (HD2023)\"\n",
        "\n",
        "print(\"Project folders ready:\")\n",
        "print(\" - RAW   :\", RAW_DIR)\n",
        "print(\" - CACHE :\", CACHE_DIR)\n",
        "print(\" - OUTPUT:\", OUTPUT_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwldbFMVs0TQ",
        "outputId": "a3674981-ece8-4f45-b455-818fe9b3a9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project folders ready:\n",
            " - RAW   : /content/drive/MyDrive/T2_IPEDS_Crosswalk/data/raw\n",
            " - CACHE : /content/drive/MyDrive/T2_IPEDS_Crosswalk/data/cache\n",
            " - OUTPUT: /content/drive/MyDrive/T2_IPEDS_Crosswalk/data/outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 2) Canonical name standardization\n",
        "# ============================================================\n",
        "\n",
        "def standardize_name(name: str) -> str:\n",
        "    if pd.isna(name) or name is None:\n",
        "        return \"\"\n",
        "    name = str(name).lower()\n",
        "\n",
        "    name = name.replace(\" at \", \"-\").replace(\" & \", \" and \")\n",
        "    name = re.sub(r\"\\s*-\\s*\", \"-\", name)\n",
        "    name = re.sub(r\"-+\", \"-\", name)\n",
        "\n",
        "    name = name.replace(\" uni \", \" university \").replace(\" univ \", \" university \")\n",
        "    name = name.replace(\" coll \", \" college \").replace(\" clg \", \" college \")\n",
        "    name = name.replace(\" inst \", \" institute \")\n",
        "    name = name.replace(\" tech \", \" technical \")\n",
        "    name = name.replace(\" st \", \" saint \").replace(\" mt \", \" mount \")\n",
        "    name = name.replace(\" dept \", \" department \")\n",
        "    name = name.replace(\" isd \", \" independent school district \")\n",
        "    name = name.replace(\" sch \", \" school \")\n",
        "    name = name.replace(\" of \", \" \")\n",
        "\n",
        "    if name.startswith(\"the \"):\n",
        "        name = name[4:]\n",
        "\n",
        "    name = re.sub(r\"[^a-z0-9\\s-]\", \"\", name)\n",
        "    name = re.sub(r\"\\s+\", \" \", name)\n",
        "    name = name.strip().strip(\"-\")\n",
        "    return name\n",
        "\n"
      ],
      "metadata": {
        "id": "3bLE3m_Cs9HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 3) Build or load Title II program universe\n",
        "# ============================================================\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def download_excel_sheet(\n",
        "    url: str,\n",
        "    sheet_name: str = TITLEII_SHEET_NAME,\n",
        "    timeout: int = 30\n",
        ") -> pd.DataFrame:\n",
        "    headers = {\"User-Agent\": \"titleII-ipeds-crosswalk/1.0 (research; colab)\"}\n",
        "    resp = requests.get(url, timeout=timeout, headers=headers)\n",
        "    resp.raise_for_status()\n",
        "    return pd.read_excel(\n",
        "        io.BytesIO(resp.content),\n",
        "        sheet_name=sheet_name,\n",
        "        engine=\"openpyxl\"\n",
        "    )\n",
        "\n",
        "\n",
        "def build_titleII_program_universe(urls: List[str]) -> pd.DataFrame:\n",
        "    dfs: List[pd.DataFrame] = []\n",
        "\n",
        "    for url in urls:\n",
        "        df = download_excel_sheet(url, sheet_name=TITLEII_SHEET_NAME)\n",
        "\n",
        "        report_year = int(url.split(\"/\")[-2])\n",
        "        df[\"ReportYear\"] = report_year\n",
        "\n",
        "        dfs.append(df)\n",
        "\n",
        "    # Combine all years\n",
        "    all_data = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Normalize column headers\n",
        "    all_data.columns = (\n",
        "        all_data.columns.astype(str)\n",
        "        .str.replace(\"\\u00A0\", \" \", regex=False)\n",
        "        .str.strip()\n",
        "        .str.replace(\" \", \"\", regex=False)\n",
        "    )\n",
        "\n",
        "    # Validate ProgramType\n",
        "    if \"ProgramType\" not in all_data.columns:\n",
        "        raise KeyError(\"Expected 'ProgramType' column not found in Title II data.\")\n",
        "\n",
        "    # Filter to target program types\n",
        "    filtered = all_data[\n",
        "        all_data[\"ProgramType\"].isin(TARGET_PROGRAM_TYPES)\n",
        "    ].copy()\n",
        "\n",
        "    # Validate required columns\n",
        "    required_cols = [\"State\", \"ReportYear\", \"ProgramCode\", \"Program\", \"ProgramType\"]\n",
        "    missing = [c for c in required_cols if c not in filtered.columns]\n",
        "    if missing:\n",
        "        raise KeyError(f\"Missing required Title II columns: {missing}\")\n",
        "\n",
        "    # Deterministic first appearance (earliest year)\n",
        "    filtered = filtered.sort_values(\n",
        "        by=[\"State\", \"Program\", \"ProgramType\", \"ReportYear\"]\n",
        "    )\n",
        "\n",
        "    first_instances = (\n",
        "        filtered\n",
        "        .drop_duplicates(\n",
        "            subset=[\"State\", \"Program\", \"ProgramType\"],\n",
        "            keep=\"first\"\n",
        "        )\n",
        "        .copy()   # IMPORTANT: prevents SettingWithCopyWarning\n",
        "    )\n",
        "\n",
        "    # Standardize program name and state\n",
        "    first_instances.loc[:, \"StandardizedProgram\"] = (\n",
        "        first_instances[\"Program\"].map(standardize_name)\n",
        "    )\n",
        "\n",
        "    first_instances.loc[:, \"Standardized_State\"] = (\n",
        "        first_instances[\"State\"]\n",
        "        .astype(str)\n",
        "        .str.strip()\n",
        "        .map(STATE_NAME_TO_ABBR)\n",
        "    )\n",
        "\n",
        "    # Safety check: unmapped states\n",
        "    unmapped = (\n",
        "        first_instances[first_instances[\"Standardized_State\"].isna()][\"State\"]\n",
        "        .dropna()\n",
        "        .unique()\n",
        "        .tolist()\n",
        "    )\n",
        "    if unmapped:\n",
        "        print(\"WARNING: Unmapped Title II state/territory values:\", unmapped)\n",
        "\n",
        "    # Final column selection and ordering\n",
        "    out_cols = [\n",
        "        \"State\",\n",
        "        \"Standardized_State\",\n",
        "        \"ReportYear\",\n",
        "        \"ProgramCode\",\n",
        "        \"Program\",\n",
        "        \"StandardizedProgram\",\n",
        "        \"ProgramType\",\n",
        "    ]\n",
        "\n",
        "    final_df = (\n",
        "        first_instances\n",
        "        .reindex(columns=out_cols)\n",
        "        .sort_values(\n",
        "            by=[\"Standardized_State\", \"StandardizedProgram\", \"ProgramType\"]\n",
        "        )\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    return final_df\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Load from cache or rebuild\n",
        "# ============================================================\n",
        "\n",
        "if (not REBUILD_FROM_SOURCE) and TITLEII_CACHE.exists():\n",
        "    print(f\"Loading cached Title II program universe: {TITLEII_CACHE}\")\n",
        "    titleII_universe = pd.read_csv(TITLEII_CACHE)\n",
        "else:\n",
        "    print(\"Rebuilding Title II program universe from official URLs...\")\n",
        "    titleII_universe = build_titleII_program_universe(TITLEII_URLS)\n",
        "    titleII_universe.to_csv(TITLEII_CACHE, index=False)\n",
        "    print(f\"Saved Title II cache: {TITLEII_CACHE}\")\n",
        "\n",
        "print(\"Title II universe rows:\", len(titleII_universe))\n",
        "print(titleII_universe.head())\n"
      ],
      "metadata": {
        "id": "qL3d4duMwDB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 4) Build or load IPEDS match table (official + entity + aliases)\n",
        "# ============================================================\n",
        "\n",
        "def load_ipeds_main(path: Path) -> pd.DataFrame:\n",
        "    if path.suffix.lower() in [\".xlsx\", \".xls\"]:\n",
        "        return pd.read_excel(path, dtype=str)\n",
        "    else:\n",
        "        return pd.read_csv(path, dtype=str)\n",
        "\n",
        "    needed = [IPEDS_UNITID_COL, IPEDS_OFFICIAL_NAME_COL, IPEDS_ENTITY_NAME_COL, IPEDS_ALIAS_SOURCE_COL, IPEDS_STATE_ABBR_COL]\n",
        "    missing = [c for c in needed if c not in df.columns]\n",
        "    if missing:\n",
        "        raise KeyError(f\"IPEDS CSV is missing expected columns: {missing}\")\n",
        "    return df\n",
        "\n",
        "def build_ipeds_alias_table(df_ipeds_main: pd.DataFrame) -> pd.DataFrame:\n",
        "    s = df_ipeds_main[[IPEDS_UNITID_COL, IPEDS_ALIAS_SOURCE_COL]].copy()\n",
        "    s[IPEDS_ALIAS_SOURCE_COL] = (\n",
        "        s[IPEDS_ALIAS_SOURCE_COL].fillna(\"\").astype(str).replace(\"nan\", \"\").str.rstrip(\"|\")\n",
        "    )\n",
        "\n",
        "    aliases = s.assign(MatchName=s[IPEDS_ALIAS_SOURCE_COL].str.split(\"|\")).explode(\"MatchName\")\n",
        "    aliases[\"MatchName\"] = aliases[\"MatchName\"].astype(str).str.strip()\n",
        "    aliases = aliases[aliases[\"MatchName\"] != \"\"]\n",
        "    aliases[IPEDS_UNITID_COL] = aliases[IPEDS_UNITID_COL].astype(str)\n",
        "\n",
        "    aliases[\"MatchName\"] = aliases[\"MatchName\"].map(standardize_name)\n",
        "    aliases = aliases[aliases[\"MatchName\"] != \"\"]\n",
        "    return aliases[[IPEDS_UNITID_COL, \"MatchName\"]].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "def build_ipeds_match_table(df_ipeds_main: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df_ipeds_main.copy()\n",
        "    df[IPEDS_UNITID_COL] = df[IPEDS_UNITID_COL].astype(str)\n",
        "    df[\"Standardized_State\"] = df[IPEDS_STATE_ABBR_COL].astype(str).str.upper().str.strip()\n",
        "\n",
        "    official = df[[IPEDS_UNITID_COL, IPEDS_OFFICIAL_NAME_COL, \"Standardized_State\"]].rename(columns={IPEDS_OFFICIAL_NAME_COL: \"MatchName\"})\n",
        "    entity = df[[IPEDS_UNITID_COL, IPEDS_ENTITY_NAME_COL, \"Standardized_State\"]].rename(columns={IPEDS_ENTITY_NAME_COL: \"MatchName\"})\n",
        "\n",
        "    aliases = build_ipeds_alias_table(df)\n",
        "    aliases = aliases.merge(df[[IPEDS_UNITID_COL, \"Standardized_State\"]], on=IPEDS_UNITID_COL, how=\"left\")\n",
        "\n",
        "    out = pd.concat([official, entity, aliases], ignore_index=True)\n",
        "    out[\"MatchName\"] = out[\"MatchName\"].map(standardize_name)\n",
        "    out = out[(out[\"MatchName\"] != \"\") & (out[\"Standardized_State\"] != \"\")]\n",
        "    out = out.drop_duplicates(subset=[IPEDS_UNITID_COL, \"MatchName\", \"Standardized_State\"]).reset_index(drop=True)\n",
        "    return out[[IPEDS_UNITID_COL, \"MatchName\", \"Standardized_State\"]]\n",
        "\n",
        "if (not REBUILD_FROM_SOURCE) and IPEDS_MATCH_CACHE.exists():\n",
        "    print(f\"Loading cached IPEDS match table: {IPEDS_MATCH_CACHE}\")\n",
        "    ipeds_match = pd.read_csv(IPEDS_MATCH_CACHE, dtype={IPEDS_UNITID_COL: str})\n",
        "else:\n",
        "    print(\"Building IPEDS match table...\")\n",
        "    df_ipeds_main = load_ipeds_main(IPEDS_MAIN_FILE)\n",
        "    ipeds_match = build_ipeds_match_table(df_ipeds_main)\n",
        "    ipeds_match.to_csv(IPEDS_MATCH_CACHE, index=False)\n",
        "    print(f\"Saved IPEDS match cache: {IPEDS_MATCH_CACHE}\")\n",
        "\n",
        "print(\"IPEDS match rows:\", len(ipeds_match))\n",
        "print(ipeds_match.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "3KAczSA3tr3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5) Matching: Exact within state, then fuzzy within state\n",
        "# ============================================================\n",
        "\n",
        "# --- Exact match ---\n",
        "exact = titleII_universe.merge(\n",
        "    ipeds_match,\n",
        "    left_on=[\"StandardizedProgram\", \"Standardized_State\"],\n",
        "    right_on=[\"MatchName\", \"Standardized_State\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# If multiple matches exist for the same Title II row, keep the first deterministically\n",
        "exact = exact.sort_values(by=[\"Standardized_State\", \"StandardizedProgram\", IPEDS_UNITID_COL])\n",
        "exact = exact.drop_duplicates(subset=[\"State\", \"ReportYear\", \"ProgramCode\", \"Program\", \"ProgramType\"], keep=\"first\")\n",
        "\n",
        "exact = exact.rename(columns={IPEDS_UNITID_COL: \"Matched_UnitID\"})\n",
        "exact[\"Match_Method\"] = exact[\"Matched_UnitID\"].apply(lambda x: \"ExactState\" if pd.notna(x) else pd.NA)\n",
        "\n",
        "unmatched = exact[exact[\"Matched_UnitID\"].isna()].copy()\n",
        "print(\"Exact matches:\", exact[\"Matched_UnitID\"].notna().sum())\n",
        "print(\"Unmatched after exact:\", len(unmatched))\n",
        "\n",
        "# --- Fuzzy match (state-aware) ---\n",
        "candidates = ipeds_match[[\"MatchName\", \"Standardized_State\", IPEDS_UNITID_COL]].drop_duplicates().values.tolist()\n",
        "\n",
        "def find_best_fuzzy_match_state_aware(program_name: str, program_state: str) -> Tuple[Optional[str], Optional[str], Optional[int]]:\n",
        "    if not program_name or not program_state:\n",
        "        return None, None, None\n",
        "\n",
        "    state_choices = [(nm, uid) for (nm, st, uid) in candidates if st == program_state]\n",
        "    if not state_choices:\n",
        "        return None, None, None\n",
        "\n",
        "    names = [x[0] for x in state_choices]\n",
        "    name_to_unit = {x[0]: x[1] for x in state_choices}\n",
        "\n",
        "    best = process.extractOne(program_name, names, scorer=fuzz.token_set_ratio)\n",
        "    if best is None:\n",
        "        return None, None, None\n",
        "\n",
        "    matched_name, score = best[0], best[1]\n",
        "    if score >= FUZZY_MATCH_THRESHOLD:\n",
        "        return name_to_unit.get(matched_name), matched_name, score\n",
        "    return None, None, None\n",
        "\n",
        "fuzzy_rows = []\n",
        "for _, row in unmatched.iterrows():\n",
        "    unitid, matched_name, score = find_best_fuzzy_match_state_aware(row[\"StandardizedProgram\"], row[\"Standardized_State\"])\n",
        "    if unitid:\n",
        "        fuzzy_rows.append((row[\"State\"], row[\"ReportYear\"], row[\"ProgramCode\"], row[\"Program\"], row[\"ProgramType\"], unitid, matched_name, score))\n",
        "\n",
        "df_fuzzy = pd.DataFrame(\n",
        "    fuzzy_rows,\n",
        "    columns=[\"State\", \"ReportYear\", \"ProgramCode\", \"Program\", \"ProgramType\", \"Matched_UnitID\", \"Fuzzy_Matched_IPEDS_Name\", \"Fuzzy_Match_Score\"]\n",
        ")\n",
        "\n",
        "final = exact.merge(\n",
        "    df_fuzzy,\n",
        "    on=[\"State\", \"ReportYear\", \"ProgramCode\", \"Program\", \"ProgramType\"],\n",
        "    how=\"left\",\n",
        "    suffixes=(\"\", \"_fuzzy\")\n",
        ")\n",
        "\n",
        "final[\"Matched_UnitID\"] = final[\"Matched_UnitID\"].fillna(final[\"Matched_UnitID_fuzzy\"])\n",
        "final.loc[final[\"Match_Method\"].isna() & final[\"Matched_UnitID\"].notna(), \"Match_Method\"] = \"FuzzyState\"\n",
        "final = final.drop(columns=[\"Matched_UnitID_fuzzy\"], errors=\"ignore\")\n",
        "\n",
        "print(\"Total matched (exact + fuzzy):\", final[\"Matched_UnitID\"].notna().sum())\n",
        "print(\"Total unmatched:\", final[\"Matched_UnitID\"].isna().sum())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjO3Dfl51IAT",
        "outputId": "2a7afd80-6fe5-4a10-8e34-a3ff4f3e2467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact matches: 1836\n",
            "Unmatched after exact: 1729\n",
            "Total matched (exact + fuzzy): 2805\n",
            "Total unmatched: 760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 6) Export outputs\n",
        "# ============================================================\n",
        "\n",
        "final.to_csv(CROSSWALK_OUT, index=False)\n",
        "\n",
        "unmatched_out = final[final[\"Matched_UnitID\"].isna()].copy()\n",
        "unmatched_out.to_csv(UNMATCHED_OUT, index=False)\n",
        "\n",
        "print(\"Wrote outputs:\")\n",
        "print(\" - Crosswalk :\", CROSSWALK_OUT)\n",
        "print(\" - Unmatched :\", UNMATCHED_OUT)\n",
        "\n",
        "summary = final[\"Match_Method\"].value_counts(dropna=False).to_frame(\"count\")\n",
        "print(\"\\nMatch method summary:\")\n",
        "print(summary)\n",
        "\n"
      ],
      "metadata": {
        "id": "TmyDXxFC2B3P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}